{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import types as T, functions as F, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import StorageLevel\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIN = \"YOUR_LOGIN\"  # Your gateway.st login\n",
    "APP_NAME = \"YOUR_APP_NAME\"  # Any name for your Spark-app\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_').replace(' ', '_').replace('\\\\', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = (\n",
    "    \"-Dlog4j.configuration=file://{} \"\n",
    "    \"-Dspark.hadoop.dfs.replication=1 \"\n",
    "    \"-Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\n",
    "    .format(LOG4J_PROP_FILE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template.stream(logfile=LOG_FILE).dump(LOG4J_PROP_FILE)\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(APP_NAME)\n",
    "    \n",
    "    # Master URI/configuration\n",
    "    .master(\"k8s://https://10.32.7.103:6443\")\n",
    "    \n",
    "    .config(\"spark.driver.host\", LOCAL_IP)\n",
    "    \n",
    "    # Web-UI port for your Spark-app\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    \n",
    "    # How many CPU cores allocate to driver process\n",
    "    .config(\"spark.driver.cores\", \"2\")\n",
    "    \n",
    "    # How many RAM allocate to driver process\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    \n",
    "    # How many executors to create\n",
    "    .config(\"spark.executor.instances\", \"3\")\n",
    "    \n",
    "    # How many CPU cores allocate to each executor\n",
    "    .config(\"spark.executor.cores\", '2')\n",
    "    \n",
    "    # How many RAM allocate to each executor\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    \n",
    "    # How many extra RAM allocate to each executor pod to handle with JVM overheads\n",
    "    # Total pod RAM = 'spark.executor.memory' + ('spark.executor.memory' * 'spark.kubernetes.memoryOverheadFactor')\n",
    "    .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.2\")\n",
    "    \n",
    "    # How many RAM from the pool allocate to store the data\n",
    "    # Additional info: https://spark.apache.org/docs/latest/tuning.html#memory-management-overview\n",
    "    .config(\"spark.memory.fraction\", \"0.6\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "    \n",
    "    .config(\"spark.network.timeout\", \"180s\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\n",
    "    \n",
    "    # Namespace to create executor pods. You are allowed to create pods only in your own namespace\n",
    "    .config(\"spark.kubernetes.namespace\", LOGIN)\n",
    "    \n",
    "    # Extra labels to your driver/executor pods in Kubernetes\n",
    "    .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\n",
    "    .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\n",
    "    \n",
    "    # Spark executor image\n",
    "    .config(\"spark.kubernetes.container.image\", f\"node03.st:5000/spark-executor:{LOGIN}\")\n",
    "\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "    \n",
    "    # If true - delete completed/failed pods. \n",
    "    # If your executors goes down you can set 'false' to check logs and troubleshoot your app.\n",
    "    .config(\"spark.kubernetes.executor.deleteOnTermination\", \"true\")\n",
    "    \n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"hdfs:///shared/bigdata20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itmo_posts_df = spark.read.json(f\"{DATA_PATH}/posts_api.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_posts_likes_df = spark.read.parquet(f\"{DATA_PATH}/followers_posts_likes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_posts_df = spark.read.json(f\"{DATA_PATH}/followers_posts_api_final.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
